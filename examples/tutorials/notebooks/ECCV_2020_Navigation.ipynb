{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Habitat-sim Basics for Navigation\n",
    "\n",
    "The Habitat platform relies on a number of key abstractions that model the domain of embodied agents and tasks that can be carried out in three-dimensional indoor simulation environments.\n",
    "\n",
    "- **Agent**: a virtually embodied agent (e.g., a robot) with a suite of Sensors. Can observe the environment and is capable of taking actions that change agent or environment state.\n",
    "- **Sensor**: associated with a specific Agent, capable of returning observation data from the environment at a specified frequency.\n",
    "- **Scene**: a 3D environment containing a scene mesh, objects, Agents, and Sensors.\n",
    "- **SceneGraph**: a hierarchical representation of a Scene that organizes the environment into regions and objects. Can be programmatically manipulated. All Scene components are present on the SceneGraph.\n",
    "- **Simulator**: an instance of a simulator backend. Given actions for a set of configured Agents and SceneGraphs, can update the state of the Agents and SceneGraphs, and provide observations for all active Sensors possessed by the Agents.\n",
    "\n",
    "This tutorial covers the basics of using Habitat-sim for navigation tasks, including:\n",
    "- configuration of a Simulator, Sensors, and Agents.\n",
    "- taking actions and retrieving observations\n",
    "- pathfinding and navigation on the NavMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Setup and Imports { display-mode: \"form\" }\n",
    "# @markdown (double click to see the code)\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import git\n",
    "import imageio\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# function to display the topdown map\n",
    "from PIL import Image\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "print(f\"data_path = {data_path}\")\n",
    "# @markdown Optionally configure the save path for video output:\n",
    "output_directory = os.path.join(\n",
    "    dir_path, \"examples/tutorials/nav_output/\"\n",
    ")  # @param {type:\"string\"}\n",
    "output_path = os.path.join(dir_path, output_directory)\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Observation Display Utility Function { display-mode: \"form\" }\n",
    "\n",
    "# @markdown A convenient function that displays sensor observations with matplotlib.\n",
    "\n",
    "# @markdown (double click to see the code)\n",
    "\n",
    "\n",
    "# Change to do something like this maybe: https://stackoverflow.com/a/41432704\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    do_make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    do_make_video = False\n",
    "    display = False\n",
    "\n",
    "# import the maps module alone for topdown mapping\n",
    "if display:\n",
    "    from habitat.utils.visualizations import maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Hello, World!\"\n",
    "\n",
    "Habitat simulator for navigation consists of **3** important concepts:\n",
    "- configurable embodied agents\n",
    "- multiple sensors\n",
    "- Scene: generic 3D dataset handling (e.g., Matterport, Gibson, and Replica datasets).\n",
    "\n",
    "In the 1st example, we demonstrate how to setup 1 agent with only 1 sensor (RGB visual sensor), place it in a scene, instruct it to navigate and collect the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic settings\n",
    "\n",
    "To begin with, we specify a scene we are going to load, designate a default agent, and describe a couple of basic sensor parameters, such as the type, position, resolution of the observation (width and height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# This is the scene we are going to load.\n",
    "# we support a variety of mesh formats, such as .glb, .gltf, .obj, .ply\n",
    "test_scene = os.path.join(\n",
    "    data_path, \"scene_datasets/mp3d_example/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    ")\n",
    "\n",
    "sim_settings = {\n",
    "    \"scene\": test_scene,  # Scene path\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters, relative to the agent\n",
    "    \"width\": 256,  # Spatial resolution of the observations\n",
    "    \"height\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Configurations for the simulator\n",
    "\n",
    "To run the simulator, we need to create a configuration that are understandable by our simulator.\\\n",
    "Such a configuration consists of **2** parts:\n",
    "- **One for the simulator backend.** It specifies parameters that are required to start and run the simulator. For example, the scene to be loaded, whether to load the semantic mesh, to enable physics or not. (Details: [code](https://github.com/facebookresearch/habitat-sim/blob/5820e1adc3ab238d2f564241d4705da5755542c9/src/esp/sim/Simulator.h#L44))\n",
    "- **One for the agent.** It describes parameters to initialize an agent, such as height, mass, as well as the configs for the attached sensors. User can also define the amount of displacement e.g., in a forward action and the turn angle.\n",
    "(Details: [code](https://github.com/facebookresearch/habitat-sim/blob/5820e1adc3ab238d2f564241d4705da5755542c9/src/esp/agent/Agent.h#L52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a config for the simulator.\n",
    "# It contains two parts:\n",
    "# one for the simulator backend\n",
    "# one for the agent, where you can attach a bunch of sensors\n",
    "def make_simple_cfg(settings):\n",
    "    # simulator backend\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "\n",
    "    # agent\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "\n",
    "    # In the 1st example, we attach only one sensor,\n",
    "    # a RGB visual sensor, to the agent\n",
    "    rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "    rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    rgb_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    rgb_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "\n",
    "    agent_cfg.sensor_specifications = [rgb_sensor_spec]\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "cfg = make_simple_cfg(sim_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simulator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # Needed to handle out of order cell run in Jupyter\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the agent\n",
    "\n",
    "After we initialize the simulator, we could put the agent in the scene, set and query its state, such as position and orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an agent\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "\n",
    "# Set agent state\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = np.array([-0.6, 0.0, 0.0])  # in world space\n",
    "agent.set_state(agent_state)\n",
    "\n",
    "# Get agent state\n",
    "agent_state = agent.get_state()\n",
    "print(\"agent_state: position\", agent_state.position, \"rotation\", agent_state.rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigate and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the default, discrete actions that an agent can perform\n",
    "# default action space contains 3 actions: move_forward, turn_left, and turn_right\n",
    "action_names = list(cfg.agents[sim_settings[\"default_agent\"]].action_space.keys())\n",
    "print(\"Discrete action space: \", action_names)\n",
    "\n",
    "\n",
    "def navigateAndSee(action=\"\"):\n",
    "    if action in action_names:\n",
    "        observations = sim.step(action)\n",
    "        print(\"action: \", action)\n",
    "        if display:\n",
    "            display_sample(observations[\"color_sensor\"])\n",
    "\n",
    "\n",
    "action = \"turn_right\"\n",
    "navigateAndSee(action)\n",
    "\n",
    "action = \"turn_right\"\n",
    "navigateAndSee(action)\n",
    "\n",
    "action = \"move_forward\"\n",
    "navigateAndSee(action)\n",
    "\n",
    "action = \"turn_left\"\n",
    "navigateAndSee(action)\n",
    "\n",
    "# action = \"move_backward\"   // #illegal, no such action in the default action space\n",
    "# navigateAndSee(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take away and look ahead\n",
    "\n",
    "- **The basics of Habitat-Sim**: config and start the simulator, load a scene, setup an agent with a sensor, instruct the agent to navigate and see, obtain the observations.\n",
    "- In the following section, we will present a comprehensive example to demonstrate:\n",
    "    - **different types of sensors** (\"rgb\", \"semantic\", \"depth\" etc.) and their configurations\n",
    "    - **the semantic scene**, and its annotation information\n",
    "    - how to define and config the **action space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How to Take your ~~Dragon~~ [Agent] for a (Random) Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configure Sim Settings\n",
    "\n",
    "test_scene = os.path.join(\n",
    "    data_path, \"scene_datasets/mp3d_example/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    ")\n",
    "mp3d_scene_dataset = os.path.join(\n",
    "    data_path, \"scene_datasets/mp3d_example/mp3d.scene_dataset_config.json\"\n",
    ")\n",
    "\n",
    "rgb_sensor = True  # @param {type:\"boolean\"}\n",
    "depth_sensor = True  # @param {type:\"boolean\"}\n",
    "semantic_sensor = True  # @param {type:\"boolean\"}\n",
    "\n",
    "sim_settings = {\n",
    "    \"width\": 256,  # Spatial resolution of the observations\n",
    "    \"height\": 256,\n",
    "    \"scene\": test_scene,  # Scene path\n",
    "    \"scene_dataset\": mp3d_scene_dataset,  # the scene dataset configuration files\n",
    "    \"default_agent\": 0,\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "    \"color_sensor\": rgb_sensor,  # RGB sensor\n",
    "    \"depth_sensor\": depth_sensor,  # Depth sensor\n",
    "    \"semantic_sensor\": semantic_sensor,  # Semantic sensor\n",
    "    \"seed\": 1,  # used in the random navigation\n",
    "    \"enable_physics\": False,  # kinematics only\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sim_cfg.scene_dataset_config_file = settings[\"scene_dataset\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "\n",
    "    # Note: all sensors must have the same resolution\n",
    "    sensor_specs = []\n",
    "\n",
    "    color_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_spec.uuid = \"color_sensor\"\n",
    "    color_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    color_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    color_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(color_sensor_spec)\n",
    "\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(depth_sensor_spec)\n",
    "\n",
    "    semantic_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    semantic_sensor_spec.uuid = \"semantic_sensor\"\n",
    "    semantic_sensor_spec.sensor_type = habitat_sim.SensorType.SEMANTIC\n",
    "    semantic_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    semantic_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    semantic_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(semantic_sensor_spec)\n",
    "\n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=30.0)\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=30.0)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = make_cfg(sim_settings)\n",
    "# Needed to handle out of order cell run in Jupyter\n",
    "try:  # Got to make initialization idiot proof\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scene_recur(scene, limit_output=10):\n",
    "    print(\n",
    "        f\"House has {len(scene.levels)} levels, {len(scene.regions)} regions and {len(scene.objects)} objects\"\n",
    "    )\n",
    "    print(f\"House center:{scene.aabb.center} dims:{scene.aabb.size}\")\n",
    "\n",
    "    count = 0\n",
    "    for level in scene.levels:\n",
    "        print(\n",
    "            f\"Level id:{level.id}, center:{level.aabb.center},\"\n",
    "            f\" dims:{level.aabb.size}\"\n",
    "        )\n",
    "        for region in level.regions:\n",
    "            print(\n",
    "                f\"Region id:{region.id}, category:{region.category.name()},\"\n",
    "                f\" center:{region.aabb.center}, dims:{region.aabb.size}\"\n",
    "            )\n",
    "            for obj in region.objects:\n",
    "                print(\n",
    "                    f\"Object id:{obj.id}, category:{obj.category.name()},\"\n",
    "                    f\" center:{obj.aabb.center}, dims:{obj.aabb.size}\"\n",
    "                )\n",
    "                count += 1\n",
    "                if count >= limit_output:\n",
    "                    return\n",
    "\n",
    "\n",
    "# Print semantic annotation information (id, category, bounding box details)\n",
    "# about levels, regions and objects in a hierarchical fashion\n",
    "scene = sim.semantic_scene\n",
    "print_scene_recur(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the randomness is needed when choosing the actions\n",
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "\n",
    "# Set agent state\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = np.array([-0.6, 0.0, 0.0])  # world space\n",
    "agent.set_state(agent_state)\n",
    "\n",
    "# Get agent state\n",
    "agent_state = agent.get_state()\n",
    "print(\"agent_state: position\", agent_state.position, \"rotation\", agent_state.rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_frames = 0\n",
    "action_names = list(cfg.agents[sim_settings[\"default_agent\"]].action_space.keys())\n",
    "\n",
    "max_frames = 5\n",
    "\n",
    "while total_frames < max_frames:\n",
    "    action = random.choice(action_names)\n",
    "    print(\"action\", action)\n",
    "    observations = sim.step(action)\n",
    "    rgb = observations[\"color_sensor\"]\n",
    "    semantic = observations[\"semantic_sensor\"]\n",
    "    depth = observations[\"depth_sensor\"]\n",
    "\n",
    "    if display:\n",
    "        display_sample(rgb, semantic, depth)\n",
    "\n",
    "    total_frames += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the NavMesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Why do we need the NavMesh?\n",
    "\n",
    "In the previous sections, we took navigation constraints and collision response for granted. By default, this is enabled in the discrete Habitat-sim action space we demonstrated. However, when directly modifying the agent state, the agent will sense neither the obstacles nor the boundary of the scene when taking actions. We need to introduce a mechanism, light and fast, to enforce such constraints. This section will provide more details on that method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habitat-sim provides pathfinding and navigability constraints via integration with [Recast Navigation | Detour](https://github.com/recastnavigation/recastnavigation) through the [nav module](https://aihabitat.org/docs/habitat-sim/habitat_sim.nav.html).\n",
    "\n",
    "This tutorial section demonstrates loading, recomputing, and saving a NavMesh for a static scene as well as using it explicitly for discrete and continuous navigation tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is a NavMesh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A navigation mesh (NavMesh) is a collection of two-dimensional convex polygons (i.e., a polygon mesh) that define which areas of an environment are traversable by an agent with a particular embodiment. In other words, an agent could freely navigate around within these areas unobstructed by objects, walls, gaps, overhangs, or other barriers that are part of the environment. Adjacent polygons are connected to each other in a graph enabling efficient pathfinding algorithms to chart routes between points on the NavMesh as visualized below.\n",
    "<div>\n",
    "<img src=\"https://github.com/recastnavigation/recastnavigation/raw/main/Docs/Images/screenshot.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Using a NavMesh approximation of navigability, an agent is embodied as a rigid cylinder aligned with the gravity direction. The NavMesh is then computed by voxelizing the static scene and generating polygons on the top surfaces of solid voxels where the cylinder would sit without intersection or overhanging and respecting configured constraints such as maximum climbable slope and step-height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NavMesh utilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "###Visualizing the NavMesh: Topdown Map\n",
    "\n",
    "The PathFinder API makes it easy to produce a topdown map of navigability in a scene. Since the NavMesh is a 3D mesh, and scenes can have multiple floors or levels vertically, we need to slice the NavMesh at specific world height (y coordinate). The map is then generated by sampling the NavMesh at a configurable resolution (meters_per_pixel) with 0.5 meters of vertical slack.\n",
    "\n",
    "The following example cell defines a matplotlib function to display a top down map with optional key points overlay. It then generates a topdown map of the current scene using the minimum y coordinate of the scene bounding box as the height, or an optionally configured custom height. Note that this height is in scene global coordinates, so we cannot assume that 0 is the bottom floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# convert 3d points to 2d topdown coordinates\n",
    "def convert_points_to_topdown(pathfinder, points, meters_per_pixel):\n",
    "    points_topdown = []\n",
    "    bounds = pathfinder.get_bounds()\n",
    "    for point in points:\n",
    "        # convert 3D x,z to topdown x,y\n",
    "        px = (point[0] - bounds[0][0]) / meters_per_pixel\n",
    "        py = (point[2] - bounds[0][2]) / meters_per_pixel\n",
    "        points_topdown.append(np.array([px, py]))\n",
    "    return points_topdown\n",
    "\n",
    "\n",
    "# display a topdown map with matplotlib\n",
    "def display_map(topdown_map, key_points=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(topdown_map)\n",
    "    # plot points on map\n",
    "    if key_points is not None:\n",
    "        for point in key_points:\n",
    "            plt.plot(point[0], point[1], marker=\"o\", markersize=10, alpha=0.8)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "# @markdown ###Configure Example Parameters:\n",
    "# @markdown Configure the map resolution:\n",
    "meters_per_pixel = 0.1  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
    "# @markdown ---\n",
    "# @markdown Customize the map slice height (global y coordinate):\n",
    "custom_height = False  # @param {type:\"boolean\"}\n",
    "height = 1  # @param {type:\"slider\", min:-10, max:10, step:0.1}\n",
    "# @markdown If not using custom height, default to scene lower limit.\n",
    "# @markdown (Cell output provides scene height range from bounding box for reference.)\n",
    "\n",
    "print(\"The NavMesh bounds are: \" + str(sim.pathfinder.get_bounds()))\n",
    "if not custom_height:\n",
    "    # get bounding box minimum elevation for automatic height\n",
    "    height = sim.pathfinder.get_bounds()[0][1]\n",
    "\n",
    "if not sim.pathfinder.is_loaded:\n",
    "    print(\"Pathfinder not initialized, aborting.\")\n",
    "else:\n",
    "    # @markdown You can get the topdown map directly from the Habitat-sim API with *PathFinder.get_topdown_view*.\n",
    "    # This map is a 2D boolean array\n",
    "    sim_topdown_map = sim.pathfinder.get_topdown_view(meters_per_pixel, height)\n",
    "\n",
    "    if display:\n",
    "        # @markdown Alternatively, you can process the map using the Habitat-Lab [maps module](https://github.com/facebookresearch/habitat-lab/blob/main/habitat/utils/visualizations/maps.py)\n",
    "        hablab_topdown_map = maps.get_topdown_map(\n",
    "            sim.pathfinder, height, meters_per_pixel=meters_per_pixel\n",
    "        )\n",
    "        recolor_map = np.array(\n",
    "            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "        )\n",
    "        hablab_topdown_map = recolor_map[hablab_topdown_map]\n",
    "        print(\"Displaying the raw map from get_topdown_view:\")\n",
    "        display_map(sim_topdown_map)\n",
    "        print(\"Displaying the map from the Habitat-Lab maps module:\")\n",
    "        display_map(hablab_topdown_map)\n",
    "\n",
    "        # easily save a map to file:\n",
    "        map_filename = os.path.join(output_path, \"top_down_map.png\")\n",
    "        imageio.imsave(map_filename, hablab_topdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ## Querying the NavMesh\n",
    "\n",
    "if not sim.pathfinder.is_loaded:\n",
    "    print(\"Pathfinder not initialized, aborting.\")\n",
    "else:\n",
    "    # @markdown NavMesh area and bounding box can be queried via *navigable_area* and *get_bounds* respectively.\n",
    "    print(\"NavMesh area = \" + str(sim.pathfinder.navigable_area))\n",
    "    print(\"Bounds = \" + str(sim.pathfinder.get_bounds()))\n",
    "\n",
    "    # @markdown A random point on the NavMesh can be queried with *get_random_navigable_point*.\n",
    "    pathfinder_seed = 1  # @param {type:\"integer\"}\n",
    "    sim.pathfinder.seed(pathfinder_seed)\n",
    "    nav_point = sim.pathfinder.get_random_navigable_point()\n",
    "    print(\"Random navigable point : \" + str(nav_point))\n",
    "    print(\"Is point navigable? \" + str(sim.pathfinder.is_navigable(nav_point)))\n",
    "\n",
    "    # @markdown The radius of the minimum containing circle (with vertex centroid origin) for the isolated navigable island of a point can be queried with *island_radius*.\n",
    "    # @markdown This is analogous to the size of the point's connected component and can be used to check that a queried navigable point is on an interesting surface (e.g. the floor), rather than a small surface (e.g. a table-top).\n",
    "    print(\"Nav island radius : \" + str(sim.pathfinder.island_radius(nav_point)))\n",
    "\n",
    "    # @markdown The closest boundary point can also be queried (within some radius).\n",
    "    max_search_radius = 2.0  # @param {type:\"number\"}\n",
    "    print(\n",
    "        \"Distance to obstacle: \"\n",
    "        + str(sim.pathfinder.distance_to_closest_obstacle(nav_point, max_search_radius))\n",
    "    )\n",
    "    hit_record = sim.pathfinder.closest_obstacle_surface_point(\n",
    "        nav_point, max_search_radius\n",
    "    )\n",
    "    print(\"Closest obstacle HitRecord:\")\n",
    "    print(\" point: \" + str(hit_record.hit_pos))\n",
    "    print(\" normal: \" + str(hit_record.hit_normal))\n",
    "    print(\" distance: \" + str(hit_record.hit_dist))\n",
    "\n",
    "    vis_points = [nav_point]\n",
    "\n",
    "    # HitRecord will have infinite distance if no valid point was found:\n",
    "    if math.isinf(hit_record.hit_dist):\n",
    "        print(\"No obstacle found within search radius.\")\n",
    "    else:\n",
    "        # @markdown Points near the boundary or above the NavMesh can be snapped onto it.\n",
    "        perturbed_point = hit_record.hit_pos - hit_record.hit_normal * 0.2\n",
    "        print(\"Perturbed point : \" + str(perturbed_point))\n",
    "        print(\n",
    "            \"Is point navigable? \" + str(sim.pathfinder.is_navigable(perturbed_point))\n",
    "        )\n",
    "        snapped_point = sim.pathfinder.snap_point(perturbed_point)\n",
    "        print(\"Snapped point : \" + str(snapped_point))\n",
    "        print(\"Is point navigable? \" + str(sim.pathfinder.is_navigable(snapped_point)))\n",
    "        vis_points.append(snapped_point)\n",
    "\n",
    "    # @markdown ---\n",
    "    # @markdown ### Visualization\n",
    "    # @markdown Running this cell generates a topdown visualization of the NavMesh with sampled points overlaid.\n",
    "    meters_per_pixel = 0.1  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
    "\n",
    "    if display:\n",
    "        xy_vis_points = convert_points_to_topdown(\n",
    "            sim.pathfinder, vis_points, meters_per_pixel\n",
    "        )\n",
    "        # use the y coordinate of the sampled nav_point for the map height slice\n",
    "        top_down_map = maps.get_topdown_map(\n",
    "            sim.pathfinder, height=nav_point[1], meters_per_pixel=meters_per_pixel\n",
    "        )\n",
    "        recolor_map = np.array(\n",
    "            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "        )\n",
    "        top_down_map = recolor_map[top_down_map]\n",
    "        print(\"\\nDisplay the map with key_point overlay:\")\n",
    "        display_map(top_down_map, key_points=xy_vis_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @markdown ## Pathfinding Queries on NavMesh\n",
    "\n",
    "# @markdown The shortest path between valid points on the NavMesh can be queried as shown in this example.\n",
    "\n",
    "# @markdown With a valid PathFinder instance:\n",
    "if not sim.pathfinder.is_loaded:\n",
    "    print(\"Pathfinder not initialized, aborting.\")\n",
    "else:\n",
    "    seed = 4  # @param {type:\"integer\"}\n",
    "    sim.pathfinder.seed(seed)\n",
    "\n",
    "    # fmt off\n",
    "    # @markdown 1. Sample valid points on the NavMesh for agent spawn location and pathfinding goal.\n",
    "    # fmt on\n",
    "    sample1 = sim.pathfinder.get_random_navigable_point()\n",
    "    sample2 = sim.pathfinder.get_random_navigable_point()\n",
    "\n",
    "    # @markdown 2. Use ShortestPath module to compute path between samples.\n",
    "    path = habitat_sim.ShortestPath()\n",
    "    path.requested_start = sample1\n",
    "    path.requested_end = sample2\n",
    "    found_path = sim.pathfinder.find_path(path)\n",
    "    geodesic_distance = path.geodesic_distance\n",
    "    path_points = path.points\n",
    "    # @markdown - Success, geodesic path length, and 3D points can be queried.\n",
    "    print(\"found_path : \" + str(found_path))\n",
    "    print(\"geodesic_distance : \" + str(geodesic_distance))\n",
    "    print(\"path_points : \" + str(path_points))\n",
    "\n",
    "    # @markdown 3. Display trajectory (if found) on a topdown map of ground floor\n",
    "    if found_path:\n",
    "        meters_per_pixel = 0.025\n",
    "        height = sim.scene_aabb.y().min\n",
    "        if display:\n",
    "            top_down_map = maps.get_topdown_map(\n",
    "                sim.pathfinder, height, meters_per_pixel=meters_per_pixel\n",
    "            )\n",
    "            recolor_map = np.array(\n",
    "                [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "            )\n",
    "            top_down_map = recolor_map[top_down_map]\n",
    "            grid_dimensions = (top_down_map.shape[0], top_down_map.shape[1])\n",
    "            # convert world trajectory points to maps module grid points\n",
    "            trajectory = [\n",
    "                maps.to_grid(\n",
    "                    path_point[2],\n",
    "                    path_point[0],\n",
    "                    grid_dimensions,\n",
    "                    pathfinder=sim.pathfinder,\n",
    "                )\n",
    "                for path_point in path_points\n",
    "            ]\n",
    "            grid_tangent = mn.Vector2(\n",
    "                trajectory[1][1] - trajectory[0][1], trajectory[1][0] - trajectory[0][0]\n",
    "            )\n",
    "            path_initial_tangent = grid_tangent / grid_tangent.length()\n",
    "            initial_angle = math.atan2(path_initial_tangent[0], path_initial_tangent[1])\n",
    "            # draw the agent and trajectory on the map\n",
    "            maps.draw_path(top_down_map, trajectory)\n",
    "            maps.draw_agent(\n",
    "                top_down_map, trajectory[0], initial_angle, agent_radius_px=8\n",
    "            )\n",
    "            print(\"\\nDisplay the map with agent and path overlay:\")\n",
    "            display_map(top_down_map)\n",
    "\n",
    "        # @markdown 4. (optional) Place agent and render images at trajectory points (if found).\n",
    "        display_path_agent_renders = True  # @param{type:\"boolean\"}\n",
    "        if display_path_agent_renders:\n",
    "            print(\"Rendering observations at path points:\")\n",
    "            tangent = path_points[1] - path_points[0]\n",
    "            agent_state = habitat_sim.AgentState()\n",
    "            for ix, point in enumerate(path_points):\n",
    "                if ix < len(path_points) - 1:\n",
    "                    tangent = path_points[ix + 1] - point\n",
    "                    agent_state.position = point\n",
    "                    tangent_orientation_matrix = mn.Matrix4.look_at(\n",
    "                        point, point + tangent, np.array([0, 1.0, 0])\n",
    "                    )\n",
    "                    tangent_orientation_q = mn.Quaternion.from_matrix(\n",
    "                        tangent_orientation_matrix.rotation()\n",
    "                    )\n",
    "                    agent_state.rotation = utils.quat_from_magnum(tangent_orientation_q)\n",
    "                    agent.set_state(agent_state)\n",
    "\n",
    "                    observations = sim.get_sensor_observations()\n",
    "                    rgb = observations[\"color_sensor\"]\n",
    "                    semantic = observations[\"semantic_sensor\"]\n",
    "                    depth = observations[\"depth_sensor\"]\n",
    "\n",
    "                    if display:\n",
    "                        display_sample(rgb, semantic, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading a NavMesh for a scene:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To load a pre-computed NavMesh for a scene, simply include it with the scene asset you are loading using the `.navmesh` file-ending.\n",
    "\n",
    "E.g.\n",
    "```\n",
    "habitat-test-scenes/\n",
    "    apartment_1.glb\n",
    "    apartment_1.navmesh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new simulator with the apartment_1 scene\n",
    "# this will automatically load the accompanying .navmesh file\n",
    "sim_settings[\"scene\"] = os.path.join(\n",
    "    data_path, \"scene_datasets/habitat-test-scenes/apartment_1.glb\"\n",
    ")\n",
    "cfg = make_cfg(sim_settings)\n",
    "try:  # Got to make initialization idiot proof\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)\n",
    "\n",
    "# the navmesh can also be explicitly loaded\n",
    "sim.pathfinder.load_nav_mesh(\n",
    "    os.path.join(data_path, \"scene_datasets/habitat-test-scenes/apartment_1.navmesh\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompute the NavMesh at runtime\n",
    "\n",
    "When computing the NavMesh at runtime, configuration options are available to customize the result based on the intended use case.\n",
    "\n",
    "To learn more, visit [this blog](http://digestingduck.blogspot.com/2009/08/recast-settings-uncovered.html) by the author of Recast.\n",
    "\n",
    "These settings include (all quantities in world units):\n",
    "- **Voxelization parameters**:\n",
    "\n",
    "  *Decrease these for better accuracy at the cost of higher compute cost.*\n",
    "\n",
    "  **Note:** most continuous parameters are converted to multiples of cell dimensions, so these should be compatible values for best accuracy.\n",
    "  - **cell_size** - xz-plane voxel dimensions. [Limit: >= 0]\n",
    "  - **cell_height** - y-axis voxel dimension. [Limit: >= 0]\n",
    "\n",
    "- **Agent parameters**:\n",
    "\n",
    "  - **agent_height** - Height of the agent. Used to cull navigable cells with obstructions.\n",
    "  - **agent_radius** - Radius of the agent. Used as distance to erode/shrink the computed heightfield. [Limit: >=0]\n",
    "  - **agent_max_climb** - Maximum ledge height that is considered to still be traversable. [Limit: >=0]\n",
    "  - **agent_max_slope** - The maximum slope that is considered navigable. [Limits: 0 <= value < 85] [Units: Degrees]\n",
    "\n",
    "- **Navigable area filtering options** (default active):\n",
    "  - **filter_low_hanging_obstacles** - Marks navigable spans as non-navigable if the clearance above the span is less than the specified height.\n",
    "  - **filter_ledge_spans** - Marks spans that are ledges as non-navigable. This filter reduces the impact of the overestimation of conservative voxelization so the resulting mesh will not have regions hanging in the air over ledges.\n",
    "  - **filter_walkable_low_height_spans** - Marks navigable spans as non-navigable if the clearance above the span is less than the specified height. Allows the formation of navigable regions that will flow over low lying objects such as curbs, and up structures such as stairways.\n",
    "\n",
    "- **Detail mesh generation parameters**:\n",
    "  - **region_min_size** - Minimum number of cells allowed to form isolated island areas.\n",
    "  - **region_merge_size** - Any 2-D regions with a smaller span (cell count) will, if possible, be merged with larger regions. [Limit: >=0]\n",
    "  - **edge_max_len** - The maximum allowed length for contour edges along the border of the mesh. Extra vertices will be inserted as needed to keep contour edges below this length. A value of zero effectively disables this feature. [Limit: >=0] [ / cell_size]\n",
    "  - **edge_max_error** - The maximum distance a simplified contour's border edges should deviate the original raw contour. [Limit: >=0]\n",
    "  - **verts_per_poly** - The maximum number of vertices allowed for polygons generated during the contour to polygon conversion process.[Limit: >= 3]\n",
    "  - **detail_sample_dist** - Sets the sampling distance to use when generating the detail mesh. (For height detail only.) [Limits: 0 or >= 0.9] [x cell_size]\n",
    "  - **detail_sample_max_error** - The maximum distance the detail mesh surface should deviate from heightfield data. (For height detail only.) [Limit: >=0] [x cell_height]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @markdown ## Recompute NavMesh:\n",
    "\n",
    "# @markdown Take a moment to edit some parameters and visualize the resulting NavMesh. Consider agent_radius and agent_height as the most impactful starting point. Note that large variations from the defaults for these parameters (e.g. in the case of very small agents) may be better supported by additional changes to cell_size and cell_height.\n",
    "navmesh_settings = habitat_sim.NavMeshSettings()\n",
    "\n",
    "# @markdown Choose Habitat-sim defaults (e.g. for point-nav tasks), or custom settings.\n",
    "use_custom_settings = False  # @param {type:\"boolean\"}\n",
    "sim.navmesh_visualization = True  # @param {type:\"boolean\"}\n",
    "navmesh_settings.set_defaults()\n",
    "if use_custom_settings:\n",
    "    # fmt: off\n",
    "    #@markdown ---\n",
    "    #@markdown ## Configure custom settings (if use_custom_settings):\n",
    "    #@markdown Configure the following NavMeshSettings for customized NavMesh recomputation.\n",
    "    #@markdown **Voxelization parameters**:\n",
    "    navmesh_settings.cell_size = 0.05 #@param {type:\"slider\", min:0.01, max:0.2, step:0.01}\n",
    "    #default = 0.05\n",
    "    navmesh_settings.cell_height = 0.2 #@param {type:\"slider\", min:0.01, max:0.4, step:0.01}\n",
    "    #default = 0.2\n",
    "\n",
    "    #@markdown **Agent parameters**:\n",
    "    navmesh_settings.agent_height = 1.5 #@param {type:\"slider\", min:0.01, max:3.0, step:0.01}\n",
    "    #default = 1.5\n",
    "    navmesh_settings.agent_radius = 0.1 #@param {type:\"slider\", min:0.01, max:0.5, step:0.01}\n",
    "    #default = 0.1\n",
    "    navmesh_settings.agent_max_climb = 0.2 #@param {type:\"slider\", min:0.01, max:0.5, step:0.01}\n",
    "    #default = 0.2\n",
    "    navmesh_settings.agent_max_slope = 45 #@param {type:\"slider\", min:0, max:85, step:1.0}\n",
    "    # default = 45.0\n",
    "    # fmt: on\n",
    "    # @markdown **Navigable area filtering options**:\n",
    "    navmesh_settings.filter_low_hanging_obstacles = True  # @param {type:\"boolean\"}\n",
    "    # default = True\n",
    "    navmesh_settings.filter_ledge_spans = True  # @param {type:\"boolean\"}\n",
    "    # default = True\n",
    "    navmesh_settings.filter_walkable_low_height_spans = True  # @param {type:\"boolean\"}\n",
    "    # default = True\n",
    "\n",
    "    # fmt: off\n",
    "    #@markdown **Detail mesh generation parameters**:\n",
    "    #@markdown For more details on the effects\n",
    "    navmesh_settings.region_min_size = 20 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "    #default = 20\n",
    "    navmesh_settings.region_merge_size = 20 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "    #default = 20\n",
    "    navmesh_settings.edge_max_len = 12.0 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "    #default = 12.0\n",
    "    navmesh_settings.edge_max_error = 1.3 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
    "    #default = 1.3\n",
    "    navmesh_settings.verts_per_poly = 6.0 #@param {type:\"slider\", min:3, max:6, step:1}\n",
    "    #default = 6.0\n",
    "    navmesh_settings.detail_sample_dist = 6.0 #@param {type:\"slider\", min:0, max:10.0, step:0.1}\n",
    "    #default = 6.0\n",
    "    navmesh_settings.detail_sample_max_error = 1.0 #@param {type:\"slider\", min:0, max:10.0, step:0.1}\n",
    "    # default = 1.0\n",
    "    # fmt: on\n",
    "\n",
    "    # @markdown **Include STATIC Objects**:\n",
    "    # @markdown Optionally include all instanced RigidObjects with STATIC MotionType as NavMesh constraints.\n",
    "    navmesh_settings.include_static_objects = True  # @param {type:\"boolean\"}\n",
    "    # default = False\n",
    "\n",
    "navmesh_success = sim.recompute_navmesh(sim.pathfinder, navmesh_settings)\n",
    "\n",
    "if not navmesh_success:\n",
    "    print(\"Failed to build the navmesh! Try different parameters?\")\n",
    "else:\n",
    "    # @markdown ---\n",
    "    # @markdown **Agent parameters**:\n",
    "\n",
    "    agent_state = sim.agents[0].get_state()\n",
    "    set_random_valid_state = False  # @param {type:\"boolean\"}\n",
    "    seed = 5  # @param {type:\"integer\"}\n",
    "    sim.seed(seed)\n",
    "    orientation = 0\n",
    "    if set_random_valid_state:\n",
    "        agent_state.position = sim.pathfinder.get_random_navigable_point()\n",
    "        orientation = random.random() * math.pi * 2.0\n",
    "    # @markdown Optionally configure the agent state (overrides random state):\n",
    "    set_agent_state = True  # @param {type:\"boolean\"}\n",
    "    try_to_make_valid = True  # @param {type:\"boolean\"}\n",
    "    if set_agent_state:\n",
    "        pos_x = 0  # @param {type:\"number\"}\n",
    "        pos_y = 0  # @param {type:\"number\"}\n",
    "        pos_z = 0.0  # @param {type:\"number\"}\n",
    "        # @markdown Y axis rotation (radians):\n",
    "        orientation = 1.56  # @param {type:\"number\"}\n",
    "        agent_state.position = np.array([pos_x, pos_y, pos_z])\n",
    "        if try_to_make_valid:\n",
    "            snapped_point = np.array(sim.pathfinder.snap_point(agent_state.position))\n",
    "            if not np.isnan(np.sum(snapped_point)):\n",
    "                print(\"Successfully snapped point to: \" + str(snapped_point))\n",
    "                agent_state.position = snapped_point\n",
    "    if set_agent_state or set_random_valid_state:\n",
    "        agent_state.rotation = utils.quat_from_magnum(\n",
    "            mn.Quaternion.rotation(-mn.Rad(orientation), mn.Vector3(0, 1.0, 0))\n",
    "        )\n",
    "        sim.agents[0].set_state(agent_state)\n",
    "\n",
    "    agent_state = sim.agents[0].get_state()\n",
    "    print(\"Agent state: \" + str(agent_state))\n",
    "    print(\" position = \" + str(agent_state.position))\n",
    "    print(\" rotation = \" + str(agent_state.rotation))\n",
    "    print(\" orientation (about Y) = \" + str(orientation))\n",
    "\n",
    "    observations = sim.get_sensor_observations()\n",
    "    rgb = observations[\"color_sensor\"]\n",
    "    semantic = observations[\"semantic_sensor\"]\n",
    "    depth = observations[\"depth_sensor\"]\n",
    "\n",
    "    if display:\n",
    "        display_sample(rgb, semantic, depth)\n",
    "        # @markdown **Map parameters**:\n",
    "        # fmt: off\n",
    "        meters_per_pixel = 0.025  # @param {type:\"slider\", min:0.01, max:0.1, step:0.005}\n",
    "        # fmt: on\n",
    "        agent_pos = agent_state.position\n",
    "        # topdown map at agent position\n",
    "        top_down_map = maps.get_topdown_map(\n",
    "            sim.pathfinder, height=agent_pos[1], meters_per_pixel=meters_per_pixel\n",
    "        )\n",
    "        recolor_map = np.array(\n",
    "            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "        )\n",
    "        top_down_map = recolor_map[top_down_map]\n",
    "        grid_dimensions = (top_down_map.shape[0], top_down_map.shape[1])\n",
    "        # convert world agent position to maps module grid point\n",
    "        agent_grid_pos = maps.to_grid(\n",
    "            agent_pos[2], agent_pos[0], grid_dimensions, pathfinder=sim.pathfinder\n",
    "        )\n",
    "        agent_forward = utils.quat_to_magnum(\n",
    "            sim.agents[0].get_state().rotation\n",
    "        ).transform_vector(mn.Vector3(0, 0, -1.0))\n",
    "        agent_orientation = math.atan2(agent_forward[0], agent_forward[2])\n",
    "        # draw the agent and trajectory on the map\n",
    "        maps.draw_agent(\n",
    "            top_down_map, agent_grid_pos, agent_orientation, agent_radius_px=8\n",
    "        )\n",
    "        print(\"\\nDisplay topdown map with agent:\")\n",
    "        display_map(top_down_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ##Saving the NavMesh\n",
    "\n",
    "# fmt: off\n",
    "# @markdown An existing NavMesh can be saved with *Pathfinder.save_nav_mesh(filename)*\n",
    "if sim.pathfinder.is_loaded:\n",
    "    navmesh_save_path = os.path.join(data_path, \"test_saving.navmesh\") #@param {type:\"string\"}\n",
    "    sim.pathfinder.save_nav_mesh(navmesh_save_path)\n",
    "    print('Saved NavMesh to \"' + navmesh_save_path + '\"')\n",
    "    sim.pathfinder.load_nav_mesh(navmesh_save_path)\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Taking Actions on the NavMesh:\n",
    "\n",
    "The following example demonstrates taking random agent actions on the NavMesh. Both continuous and discrete action spaces are available. Sliding vs. non-sliding scenarios are compared.\n",
    "\n",
    "## What is sliding?\n",
    "Most game engines allow agents to slide along obstacles when commanding actions which collide with the environment. While this is a reasonable behavior in games, it does not accurately reflect the result of collisions between robotic agents and the environment.\n",
    "\n",
    "We note that **allowing sliding** makes training easier and results in higher simulation performance, but **hurts sim-2-real transfer** of trained policies.\n",
    "\n",
    "For a more detailed exposition of this subject see our paper:\n",
    "[\"Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation\"](https://arxiv.org/abs/1912.06321).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Discrete and Continuous Navigation:\n",
    "\n",
    "# @markdown Take moment to run this cell a couple times and note the differences between discrete and continuous navigation with and without sliding.\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Set example parameters:\n",
    "seed = 7  # @param {type:\"integer\"}\n",
    "# @markdown Optionally navigate on the currently configured scene and NavMesh instead of re-loading with defaults:\n",
    "use_current_scene = False  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "sim_settings[\"seed\"] = seed\n",
    "if not use_current_scene:\n",
    "    # reload a default nav scene\n",
    "    sim_settings[\"scene\"] = os.path.join(\n",
    "        data_path, \"scene_datasets/mp3d_example/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    "    )\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    try:  # make initialization Colab cell order proof\n",
    "        sim.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "# set new initial state\n",
    "sim.initialize_agent(agent_id=0)\n",
    "agent = sim.agents[0]\n",
    "\n",
    "# @markdown Seconds to simulate:\n",
    "sim_time = 10  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown Optional continuous action space parameters:\n",
    "continuous_nav = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# defaults for discrete control\n",
    "# control frequency (actions/sec):\n",
    "control_frequency = 3\n",
    "# observation/integration frames per action\n",
    "frame_skip = 1\n",
    "if continuous_nav:\n",
    "    control_frequency = 5  # @param {type:\"slider\", min:1, max:30, step:1}\n",
    "    frame_skip = 12  # @param {type:\"slider\", min:1, max:30, step:1}\n",
    "\n",
    "\n",
    "fps = control_frequency * frame_skip\n",
    "print(\"fps = \" + str(fps))\n",
    "control_sequence = []\n",
    "for _action in range(int(sim_time * control_frequency)):\n",
    "    if continuous_nav:\n",
    "        # allow forward velocity and y rotation to vary\n",
    "        control_sequence.append(\n",
    "            {\n",
    "                \"forward_velocity\": random.random() * 2.0,  # [0,2)\n",
    "                \"rotation_velocity\": (random.random() - 0.5) * 2.0,  # [-1,1)\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        control_sequence.append(random.choice(action_names))\n",
    "\n",
    "# create and configure a new VelocityControl structure\n",
    "vel_control = habitat_sim.physics.VelocityControl()\n",
    "vel_control.controlling_lin_vel = True\n",
    "vel_control.lin_vel_is_local = True\n",
    "vel_control.controlling_ang_vel = True\n",
    "vel_control.ang_vel_is_local = True\n",
    "\n",
    "# try 2 variations of the control experiment\n",
    "for iteration in range(2):\n",
    "    # reset observations and robot state\n",
    "    observations = []\n",
    "\n",
    "    video_prefix = \"nav_sliding\"\n",
    "    sim.config.sim_cfg.allow_sliding = True\n",
    "    # turn sliding off for the 2nd pass\n",
    "    if iteration == 1:\n",
    "        sim.config.sim_cfg.allow_sliding = False\n",
    "        video_prefix = \"nav_no_sliding\"\n",
    "\n",
    "    print(video_prefix)\n",
    "\n",
    "    # manually control the object's kinematic state via velocity integration\n",
    "    time_step = 1.0 / (frame_skip * control_frequency)\n",
    "    print(\"time_step = \" + str(time_step))\n",
    "    for action in control_sequence:\n",
    "        # apply actions\n",
    "        if continuous_nav:\n",
    "            # update the velocity control\n",
    "            # local forward is -z\n",
    "            vel_control.linear_velocity = np.array([0, 0, -action[\"forward_velocity\"]])\n",
    "            # local up is y\n",
    "            vel_control.angular_velocity = np.array([0, action[\"rotation_velocity\"], 0])\n",
    "\n",
    "        else:  # discrete action navigation\n",
    "            discrete_action = agent.agent_config.action_space[action]\n",
    "\n",
    "            did_collide = False\n",
    "            if agent.controls.is_body_action(discrete_action.name):\n",
    "                did_collide = agent.controls.action(\n",
    "                    agent.scene_node,\n",
    "                    discrete_action.name,\n",
    "                    discrete_action.actuation,\n",
    "                    apply_filter=True,\n",
    "                )\n",
    "            else:\n",
    "                for _, v in agent._sensors.items():\n",
    "                    habitat_sim.errors.assert_obj_valid(v)\n",
    "                    agent.controls.action(\n",
    "                        v.object,\n",
    "                        discrete_action.name,\n",
    "                        discrete_action.actuation,\n",
    "                        apply_filter=False,\n",
    "                    )\n",
    "\n",
    "        # simulate and collect frames\n",
    "        for _frame in range(frame_skip):\n",
    "            if continuous_nav:\n",
    "                # Integrate the velocity and apply the transform.\n",
    "                # Note: this can be done at a higher frequency for more accuracy\n",
    "                agent_state = agent.state\n",
    "                previous_rigid_state = habitat_sim.RigidState(\n",
    "                    utils.quat_to_magnum(agent_state.rotation), agent_state.position\n",
    "                )\n",
    "\n",
    "                # manually integrate the rigid state\n",
    "                target_rigid_state = vel_control.integrate_transform(\n",
    "                    time_step, previous_rigid_state\n",
    "                )\n",
    "\n",
    "                # snap rigid state to navmesh and set state to object/agent\n",
    "                # calls pathfinder.try_step or self.pathfinder.try_step_no_sliding\n",
    "                end_pos = sim.step_filter(\n",
    "                    previous_rigid_state.translation, target_rigid_state.translation\n",
    "                )\n",
    "\n",
    "                # set the computed state\n",
    "                agent_state.position = end_pos\n",
    "                agent_state.rotation = utils.quat_from_magnum(\n",
    "                    target_rigid_state.rotation\n",
    "                )\n",
    "                agent.set_state(agent_state)\n",
    "\n",
    "                # Check if a collision occurred\n",
    "                dist_moved_before_filter = (\n",
    "                    target_rigid_state.translation - previous_rigid_state.translation\n",
    "                ).dot()\n",
    "                dist_moved_after_filter = (\n",
    "                    end_pos - previous_rigid_state.translation\n",
    "                ).dot()\n",
    "\n",
    "                # NB: There are some cases where ||filter_end - end_pos|| > 0 when a\n",
    "                # collision _didn't_ happen. One such case is going up stairs.  Instead,\n",
    "                # we check to see if the the amount moved after the application of the filter\n",
    "                # is _less_ than the amount moved before the application of the filter\n",
    "                EPS = 1e-5\n",
    "                collided = (dist_moved_after_filter + EPS) < dist_moved_before_filter\n",
    "\n",
    "            # run any dynamics simulation\n",
    "            sim.step_physics(time_step)\n",
    "\n",
    "            # render observation\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "\n",
    "    print(\"frames = \" + str(len(observations)))\n",
    "    # video rendering with embedded 1st person view\n",
    "    if do_make_video:\n",
    "        # use the video utility to render the observations\n",
    "        vut.make_video(\n",
    "            observations=observations,\n",
    "            primary_obs=\"color_sensor\",\n",
    "            primary_obs_type=\"color\",\n",
    "            video_file=output_directory + \"continuous_nav\",\n",
    "            fps=fps,\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "    sim.reset()\n",
    "\n",
    "# [/embodied_agent_navmesh]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,notebooks//ipynb",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
